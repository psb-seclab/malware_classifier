from os.path import getsize
from math import log
import numpy as np
import re

class Sample(object):
    """A Sample represents a file whose contents can be parsed for features.
    
    When created, the sample reads in and stores all of the contents of the
    file in order to optimize the speed of the parsing.
    
    On top of holding the contents of the file, the Sample class can also
    maintain a cache of pertinent information in order to optimize the 
    performance of feature parsing by preventing duplicated logic. This can
    be used like so:
    
        sample = Sample('...')
        sample['Lines'] = sample._file_contents.split()
        if ('Lines' in sample):
            print(sample['Lines'])
    
    This class utilizes the following cache values:
    
        'WCount_{word}':  The number of instances found for the word {word} in 
                          the file.
        'Lines':          A list of all of the lines in the file.
        'Words_{regex}':  A list of all of the words in the file split using
                          the regex {regex}.
        
    """
    
    # Initialize a static cache to store values that may need to be reused
    # by all instances of the class.
    __cache = {}
    
    def __init__(self, file_name):
        """Initialize a new sample whose contents can be found in the file
        pointed to by the specified file path.
        """
        
        # Save the file name.
        self._file_name = file_name
        
        # Ensure the file is valid.
        try:
            with open(file_name, 'rb') as file:
                # Store the contents of the file for parsing.
                self._file_contents = file.read()
                
        except IOError:
            raise ValueError('Invalid file name %s' % file_name)
            
        # Initialize a cache to store values that may need to be reused.
        self.__cache = {}
        
    def __contains__(self, key):
        """Check if the cache contains an item for the specified key."""
        return key in self.__cache
        
    def __delitem__(self, key):
        """Clear the cached item with the specified key."""
        self.__cache.pop(key, None)
            
    def __getitem__(self, key):
        """Get the cached value for the specified key."""
        return self.__cache[key]
        
    def __setitem__(self, key, value):
        """Set the cached value for the specified key."""
        self.__cache[key] = value
        
    def file_size(self):
        """Get the size of the file."""
        return getsize(self._file_name)
        
    def lines(self):
        """Get the lines in the file."""
        
        # Cache the lines in the file.
        if (not 'Lines' in self):
            self['Lines'] = self._file_contents.split(b'\n')
            
        return self['Lines']
    
    @staticmethod
    def load_list(list_file_name):
        """Load and return all of the lines from the specified files as a list.
        
        Once a list is loaded from a file, the list is cached for subsequent
        calls.
        """
        
        # If the list is not in the cache, load in the file.
        if (not list_file_name in Sample.__cache):
            try:
                with open(list_file_name, 'rb') as file:
                    Sample.__cache[list_file_name] = file.read().split(b'\r\n')
            except IOError:
                raise IOError('Could not load the file %s' % list_file_name)
                
        return Sample.__cache[list_file_name]
        
    def num_lines(self):
        """Get the number of lines in the file."""
        return len(self.lines())
        
    def word_counts(self, *words, split_pattern=rb''):
        """Find the number of occurrences of the specified words.
        
        Each word must be bytes or bytes-like, not a string. You can convert
        a string literal to bytes by prepending a lowercase b before the
        quotation marks like so:
        
            b'Hello'
            
        If you wish to split the words in a file a certain way, you can
        specify the split_pattern as a bytes object containing a regex that
        will be used to split the file contents into a list of words.
        
        The return value will be a list of counts with each count corresponding
        to the passed in word.
        """
        
        # Find the word whose results are not already cached.
        uncached_words = set()
        for word in words:
            
            # Ensure the word's result is not cache'd.
            word_key = 'WCount_%s' % word
            if word_key in self:
                continue
            
            # Add the word to the set.
            uncached_words.add(word)
            
            # Initialize the word's count in the cache.
            self[word_key] = 0
            
        # If there are uncached patterns, they need to be counted.
        if uncached_words:
            
            # Retrieve all of the words in the file.
            #file_words = self.words()
            file_words = self.words(split_pattern) if split_pattern \
                                                   else self.words()
            
            # Iterate through every word in the file. If a word is in the
            # list of uncached words, increment its count.
            for word in file_words:
                if word and word in uncached_words:
                    word_key = 'WCount_%s' % word
                    self[word_key] += 1
        
        # Return a list of all of the counts.
        return [self['WCount_%s' % word] for word in words]
        
    def words(self, split_pattern=rb'\s+'):
        """Get the words in the file.
        
        The split_pattern is a regex string that will determine how the word
        boundaries are defined. The regex string matches whitespace by default.
        """
        
        # Cache the words in the file.
        key = 'Words_%s' % split_pattern
        if (not key in self):
            
            # Split the file contents into words.
            self[key] = re.split(split_pattern, self._file_contents)
            
        return self[key]
    
        
class ExeAsmSample(Sample):
    """An ExeAsmSample represents the assembly file for a Windows executable
    program.
    
    Since this class is a subclass of the Sample class, the ExeAsmSample has
    access to the Sample's caching features. Along with those used in the
    Sample class, this class uses the following cache values:
    
        'Opcodes': The list of opcodes whose counts in the file are used as
                   features.
                   
    """
    
    # Regex pattern used for splitting the file when counting registers.
    registers_split = rb'\s+|\,+|\[+|\]+|\++|\-+|\*+|\:+'
    
    def __init__(self, asm_file_name):
        """Initialize a new Windows Executable sample whose assembly file can
        be found at the specified file path.
        """
        # Call base constructor.
        super(ExeAsmSample, self).__init__(asm_file_name)
        
    def api_features(self):
        """Retrieve the frequency of the top 794 most frequent APIs used in 
        malicious files in list form.
        
        Each element of this list will show how many of each API is used.
        
        Files containing no APIs are highly suspicious.
        """
        
        api_list = self.apis()
        
        lines = self._hex_contents.split(b'\n')
        
        counts = {key: 0 for key in api_list}
        for i in api_list:
            for j in lines:
                if bytes(i, 'utf-8') in j:
                    counts[i] = counts.get(i, 0) + 1
        return counts.values()
        
    def features(self):
        """Get a list of features extracted from the assembly file.
        
        The list of features will have the following format:
        
            [ { Metadata Features (2) }, { Opcode Counts (93) }, 
              { Register Counts (26 )} ]
              
        """
        
        # Optimization: The word_counts method counts all of the words passed
        #               to it in only one pass through the file. This means
        #               that it is faster to count every word at once rather
        #               than splitting them among multiple calls. Since the
        #               word_counts method caches all of its results for
        #               subsequent calls, we can count everything at once to
        #               store all the results to the cache, which will be
        #               immediately retrieved when the feature methods call
        #               the word_count method again.
        #
        #               This is a tested optimization. It consistently saves
        #               about a fifth of a second, which can save about an
        #               hour when expanded to 20,000 samples.
        self.word_counts(*(ExeAsmSample.opcodes() + ExeAsmSample.registers()),
                         split_pattern=ExeAsmSample.registers_split)
                         
        features = []
        features += self.metadata_features()
        features += self.opcode_features()
        features += self.register_features()
        return features
        
    def metadata_features(self):
        """Get a list of metadata features from the file.
        
        The feature list will have the following format:
        
        [ { File Size }, { Line Count }]
        """
        return [self.file_size(), self.num_lines()]
        

    def misc_features(self):
        """Retrieve the frequency of 95 manually chosen keywords used in 
        malicious files in list form.
        
        Each element of the list is related to different trends in malware
        files. For example, 'dll' is used to show the number of imported dll's
        """
        
        misc_list = self.miscs()
        
        lines = self._hex_contents.split(b'\n')
        
        counts = {key: 0 for key in misc_list}
        for i in misc_list:
            for j in lines:
                if bytes(i, 'utf-8') in j:
                    counts[i] = counts.get(i, 0) + 1
        return counts.values()
        
    def opcode_features(self):
        """Get a list of counts for all of the predefined opcodes in the file.
        
        The list of counts will correspond to the list of opcodes found in
        opcodes.txt.
        """
        
        # Measure the counts of each of the opcodes.
        return self.word_counts(*ExeAsmSample.opcodes())
        
    def register_features(self):
        """Get a list of counts for all of the predefined registers in the
        file.
        
        The list of counts will correspond to the list of registers found in
        registers.txt.
        """
                            
        # Registers can often be grouped with brackets, colons, commas, and
        # mathematical operators. These will need to be filtered out.
        return self.word_counts(*ExeAsmSample.registers(), 
                                split_pattern=ExeAsmSample.registers_split)
    
    @staticmethod
    def apis():
        """Returns a list of all of the apis who counts in the file should be
        used as features
        """
        
        return Sample.load_list('apis.txt')
    
    @staticmethod
    def miscs():
        """Returns a list of 75 manually chosen keywords whos counts in the
        file should be used as features
        """
        
        return Sample.load_list('misc.txt')
    
    @staticmethod
    def opcodes():
        """Return a list of all of the opcodes whose counts in the file should
        be used as features.
        """
        
        # The opcodes should be stored in a file called 'opcodes.txt'
        return Sample.load_list('opcodes.txt')
        
    @staticmethod
    def registers():
        """Return a list of all of the registers whose counts in the file
        should be used as features.
        """
        
        # The registers should be stored in a file called 'registers.txt'
        return Sample.load_list('registers.txt')

def entropy(counts, window_size):
    """Retrieve the entropy for a window with the specified counts and window
    sizes.
    
    The counts should be a list of the counts for each of the distinct items
    in the window.
    
    The return value will be between 0 (order) and 8(randomness).
    """
    
    entropies = [count / window_size * log(count / window_size, 2) for count 
                 in counts if count > 0]
    
    return -sum(entropies)
    
class ExeSample(object):
    """An ExeSample represents a malware sample.
    
    Every malware sample must contain a hexadecimal dump. With that provided,
    the sample can be analyzed and have various features extracted from it.
    """
    
    def __init__(self, hex_file_name):
        """Initialize a new ExeSample whose hex dump can be found in the file
        pointed to by hex_file_name.
        """
        
        self.hex_file_name = hex_file_name
        
        # Ensure the hex file is valid.
        try:
            with open(hex_file_name) as file:
                # Store the contents of the file for parsing.
                self._hex_contents = file.read()
            
        except IOError:
            raise ValueError('Invalid file name: %s' % hex_file_name)
            
    def hex_bytes(self):
        """Return a list of all bytes contained in the hex file for the sample.
        
        The first time this functions is called, the bytes are parsed from the
        file and cached. Subsequent calls simply return the cache.
        
        NOTE: The ?? bytes are not filtered in this function.
        """
        
        # Parse the hex bytes from the file if they have not been parsed yet.
        if not hasattr(self, '_hex_bytes'):
            
            # Every two hex digits represents a single byte.
            contents = ''.join(self._hex_contents.split())
            self._hex_bytes = [contents[i:i+2] for i 
                               in range(0, len(contents), 2)]
                    
        return self._hex_bytes
        
    def hex_entropies(self, window_size=10000):
        """Calculate the entropy values for each of the windows in the hex
        file if the window has the specified size.
        
        A list of all of the entropy values calculated for each window will
        be returned.
        """
        
        hex_bytes = self.hex_bytes()
        
        # Ensure there are enough bytes for at least one window.
        if (len(hex_bytes) < window_size):
            raise ValueError(('The window size {0} exceeds the number ' +
                             'of bytes in the file: {1}')
                             .format(window_size, len(hex_bytes)))
                             
        # Retrieve the byte counts for the first window.
        window = hex_bytes[:window_size]
        
        byte_counts = dict()
        for byte in window:
            byte_counts[byte] = byte_counts.get(byte, 0) + 1
        
        # Calculate the entropy for the first window.
        entropies = [ entropy(byte_counts.values(), window_size) ]
        
        # Add the entropy values for the rest of the windows.
        for i in range(1, len(hex_bytes) - window_size):
            
            # Adjust the window count by removing from the count of the byte
            # that just got removed and adding to the count of the byte that
            # just got added.
            removed_byte = hex_bytes[i - 1]
            if (byte_counts.get(removed_byte, 0) != 0):
                byte_counts[removed_byte] -= 1
            
            added_byte = hex_bytes[i + window_size]
            byte_counts[added_byte] = byte_counts.get(added_byte, 0) + 1
            
            # Calculate the entropy for the current window's count.
            entropies.append(entropy(byte_counts.values(), window_size))
        
        return entropies
        
    def hex_entropy_stats(self, window_size=10000):
        """Retrieves statistics for the entropy values collected from the
        hex file.
        
        The list of stats returned are in the following format:
        
            [ { mean }, { variance }, { standard deviation }, 
              { Percentiles 0 - 100 } ]
        """
        
        # Retrieve the list of entropy values.
        entropies = self.hex_entropies(window_size)
        
        # Calculate various statistics for the entropy values.
        stats = [
            np.mean(entropies),
            np.var(entropies),
            np.std(entropies),
        ]
        
        # Percentiles 0 - 100
        for i in range(101):
            stats.append(np.percentile(entropies, i))
            
        return stats
        
                
    def hex_features(self):
        """Return a feature vector containing all of the features retrieved 
        from the hex file of the sample.
        
        The format of the feature vector is as follows (the numbers in
        parenthesis indicate how many features are in each section):
        
        [ { 1-gram frequencies (256) }, { hex file metadata (2) }, 
          { hex file entropy (105) }, { string length distribution (116) } ]
        """
        
        features = []
        
        features += self.hex_one_gram_frequencies()
        features += self.hex_metadata_features()
        features += self.hex_entropy_stats()
        features += self.hex_entropies(len(self.hex_bytes()))
        features += self.hex_string_length_counts(4, 120)
        
        return features
        
    def hex_file_size(self):
        """Return the size of the hex dump file in bytes."""
        return getsize(self.hex_file_name)
        
    def hex_first_address(self):
        """Return the address of the first byte sequence in the sample's
        hex file.
        """
        
        # The first word in the hex file is the address of the first byte 
        # sequence.
        first_word = self._hex_contents.split()[0]
        
        # The address is a hexadecimal value. Convert it to decimal.
        return int(first_word, 16)
            
    def hex_metadata_features(self):
        """Return a feature vector containing metadata related features
        extracted from the sample's hex dump.
        
        The feature vector will have the following format:
        
        [ {file size (bytes)}, {1st byte sequence address (decimal)} ]
        """
        return [self.hex_file_size(), self.hex_first_address()]
        
    def hex_string_length_counts(self, min, max):
        """Return a list of counts for all of the string lengths in the range
        of min - max (inclusive) characters.
        """
        
        # Retrieve all of the bytes in the hex file.
        bytes = self.hex_bytes()
        
        # The first four bytes of every 20 bytes contains an address, which
        # should not be analyzed to find a string.
        bytes = [bytes[i] for i in range(len(bytes)) if i % 20 >= 4]
        
        # Conver the hex values to decimal values.
        # NOTE: The ?? bytes are useless. Filter them out.
        ascii_values = [int(byte, 16) for byte in bytes if byte != '??']
        
        # The printable range for ASCII characters is 32 - 127. Replace
        # anything outside those bounds with a null to be filtered out.
        characters = [chr(value) if 32 <= value <= 127 else '\0' for value 
                      in ascii_values]
                                              
        # Retrieve each string by splitting up sequences of characters by
        # null characters.
        strings = ''.join(characters)
        strings = strings.split('\0')
        
        # Retrieve the length of each string.
        string_lengths = [len(string) for string in strings]
        
        # Record the requested counts.
        return [string_lengths.count(i) for i in range(min, max+1)]
        
    def hex_one_gram_frequencies(self, *one_grams):
        """Return the list of frequencies in the hex file for every specified
        1-gram value.
        
        A 1-gram has 256 possible values (0 - 255). The list returned will
        contain integer values representing the number of times its respective
        1-gram appeared in the file.
        
        If no argument is specified, the frequencies of all 256 possible 
        1-grams are found. In this scenario, the 1-grams are in ascending 
        order, meaning that 0x00 is at index 0 and 0xFF is at index 255.
        """
        
        # Every two hex digits represents one byte, which represents a 1-gram.
        # Remove the whitespace in the file and retrieve all of the possible
        # 1-grams.
        file_one_grams = [int(byte, 16) for byte in self.hex_bytes() 
                          if byte != '??']

        if not one_grams:
            one_grams = range(256)
        
        # Find the frequency of every passed 1-gram.
        frequencies = []
        
        for one_gram in one_grams:
            # Make sure the 1-gram is an integer.
            if not isinstance(one_gram, int):
                raise TypeError('Passed value must be an integer')
            
            # Make sure the 1-gram is in the correct range.
            if not (0 <= one_gram <= 255):
                raise ValueError('The value %s is not in the range 0 - 255' 
                                 % one_gram)
        
            # Count the frequency.
            frequencies.append(file_one_grams.count(one_gram))
        
        # Return the frequency of the specified 1-gram in the file.
        return frequencies