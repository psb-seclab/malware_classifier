from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

import pickle
import xgboost as xgb

###############################################################################

def load_classifier(filename):
    """Load in the classifier from the specified file."""
    
    # Load data from the file.
    with open(filename) as file:
        data = pickle.load(file)
        
    # If the file was correctly formatted, the data should contain the model,
    # the training data, and the testing data.
    model, training_features, training_labels, \
    testing_features, testing_labels = data
    
    # Recreate the classifier with the items read in from the file.
    classifier = Classifier(model)
    classifier._training_features = training_features
    classifier._training_labels = training_labels
    classifier._testing_features = testing_features
    classifier._testing_labels = testing_labels
    
    return classifier
    
###############################################################################

class Classifier(object):
    """Classifier is the base class for all types of classifiers. It defines 
    the base logic that all classes should take advantage of. In particular, 
    the base constructor can be used to handle verifying and splitting a passed 
    in data set.
    
    A Classifier takes in a set of labeled data and then allows classifying
    unlabeled data based on what it learned from the passed in labeled data.
    
    All concrete subclasses should implement the model property to return the
    model that should be used for the classification. The Classifier class
    can handle the rest of the logic.
    """
    
    file_extension = '.clsf'
    
    def __init__(self, model, features=None, labels=None, training_ratio=0.7):
        """Initialize a new Model that fits to the specified data.
        
        features is a feature matrix which should be represented by a 2D-array 
        or 2D-array-like object. Each row in the feature matrix represents a 
        sample.
        
        model is the machine learning model that should be used by the
        classifier. If data is specified, the model will be trained with the 
        data. An already trained model can be passed in by specifying no data.
        
        labels is the list of labels that correspond to each sample in the 
        feature matrix. Each element in the list corresponds to a row in the 
        feature matrix. That means that len(features) must equal len(labels).
        
        training_ratio is a float between 0 and 1 that specifies what portion 
        of the data will be used for training. The remaining portion of the 
        data will then be used for testing. The default values is 0.7.
        """
        
        # Set the model.
        self._model = model
        
        # Check if any data was passed in.
        if (features is None):
            return

        # Verify that the number of labels and features is consistent.
        if (len(features) != len(labels)):
            raise ValueError('The numbers of labels and samples are not ' + \
                             'consistent')
            
        # Split the dataset into training and testing data.
        self._training_features, self._testing_features, \
        self._training_labels, self._testing_labels = \
            train_test_split(features, labels, train_size=training_ratio)
            
        # Train the model with the training set.
        self.model.fit(self.training_features, self.training_labels)
            
    @property
    def model(self):
        """Get the trained model used by the classifier."""
        return self._model
        
    @property
    def testing_features(self):
        """Get the feature matrix used for testing the model. This will be a
        2D-array with the same length as the testing_labels.
        """
        return self._testing_features
        
    @property
    def testing_labels(self):
        """Get the list of labels used for testing the model. This will be a
        list with the same length as the training_features.
        """
        return self._testing_labels
    
    @property
    def training_features(self):
        """Get the feature matrix used for training the model. This will be a
        a 2D-array with the same length as the training_labels.
        """
        return self._training_features
    
    @property
    def training_labels(self):
        """Get the list of labels used for training the model. This will be a
        list with the same length as the training_features.
        """
        return self._training_labels
        
    def accuracy(self, testing_features=None, testing_labels=None):
        """Calculate the accuracy of the classifier by validating against a
        set of labeled test data.
        
        The labeled test data can be passed in. If no testing data is
        specified, the portion of the original data set that was reserved for
        testing will be used instead.
        
        testing_features is the feature matrix for the specified test data.
        This should be 2D-array.
        
        testing_labels is the list of labels for the specified feature matrix.
        This should be a list with the same length as testing_features.
        """
        
        # Ensure either both or neither testing parameters were specified.
        if ((testing_features is None) != (testing_labels is None)):
            raise ValueError("Must specify both testing features and labels")
            
        # Use default testing data if necessary.
        testing_features = testing_features or self.testing_features
        testing_labels = testing_labels or self.testing_labels
        
        # Determine the accuracy of the model.
        predicted = self.model.predict(testing_features)
        return accuracy_score(testing_labels, predicted)
            
    def classify(self, *samples):
        """Classifies the specified samples and returns a predicted label for
        each.
        
        Each sample should be a list of features. The number of features must
        be equal to the number of features each sample passed into the
        classifier used.
        
        A list of classification results for each sample will be returned. If
        only one sample is passed in, the return value will just be the
        classification result for the sample.
        """
            
        # Classify each sample.
        results = self.model.predict(samples)
        
        # If there is only one sample, just return the result. If there were
        # multiple samples, return the full list.
        return results if len(results) != 1 else results[0]

    def save(self, filename):
        """Save the classifier to a file to be loaded back in later.
        
        The classifier will be saved to a file with the specified name. The
        file extension for the file name will be automatically added by the
        method. 
        
        The classifier can be loaded back in using the load_classifier 
        function.
        
        The classifier file will contain the following items in the specified
        order:
            
            1. Trained Model
            2. Classifier Training Data (feature matrix followed by labels)
            3. Classifier Testing Data (feature matrix followed by labels)
        
        These items will be saved into a list, and the list will be saved into
        the file.
        """
        
        # Append the file extension to the file name.
        filename += Classifier.file_extension
        
        # Save all of the data to the file.
        data = [self.model, self.training_features, self.training_labels,
                self.testing_features, self.testing_labels]
        
        with open(filename, 'w') as file:
            pickle.dump(data, file)
    
###############################################################################

class ExeLogisticRegressionClassifier(Classifier):
    """The ExeLogisticRegressionClassifier is a classifier for Windows
    Executable Files that uses the Logistic Regression model combined with
    bagging.
    """
    
    def __init__(self, features, labels, training_ratio=0.7, 
                 bagging_iterations=10):
        """Initialize a new classifier that fits to the specified data.
        
        features is a feature matrix and should be a 2D list-like structure.
        
        labels is the list of labels and should have the same number of
        elements as the feature matrix has rows.
        
        training_ratio is a value between 0 and 1 that defines the ratio of
        the given data that will be used for training. The rest will be used
        for testing. The default is 0.7.
        
        bagging_iterations is the number of bagging iterations that will be
        performed. More iterations means longer training times but can in
        theory lead to better results. The default is 10, which is the
        generally recommended value.
        """
        
        model = LogisticRegression()
        model = BaggingClassifier(model, bagging_iterations)
        
        super(ExeLogisticRegressionClassifier, self).__init__(model, features, 
                                                              labels, 
                                                              training_ratio)

###############################################################################

class ExeXGBClassifier(Classifier):
    """The ExeXGBClassifier is a classifier for Windows Executable Files that
    uses XGBoost as the model.
    
    Note that in order for this class to be used, XGBoost will need to be
    installed. Unfortunately, this is not as simple as running a pip command
    in Windows. If you are running Windows and need to install XGBoost, take
    the following steps:
        
        1. Clone the XGBoost repository locally. The repository contains
           submodules, so you will need to do a recursive clone. You can use
           the following git command:
               
               git clone --recursive https://github.com/dmlc/xgboost
        
        2. Download the "libxgboost.dll" file from PicNet:
            
                http://www.picnet.com.au/blogs/guido/post/2016/09/22/xgboost-windows-x64-binaries-for-download/
           
           The most recent build at the time of writing and the one currently
           used is 20170106. If there are any compatibility issues, download
           this version if possible.
           
        3. In the directory you cloned the repository into, navigate to:
            
                xgboost/python-package/
                
           Paste the "libxgboost.dll" file you downloaded in this folder.
           
        4. Open a command window in the directory containing the dll and run
           the following command:
               
               python setup.py develop --user

           This will only install XGBoost for the current user, so it should
           not need Admin permissions.
           
    If these steps do not work for you, try the official installation guide:
        
        https://xgboost.readthedocs.io/en/latest/build.html
    """
           
    def __init__(self, features, labels, training_ratio=0.7):
        """Initialize a new classifier that fits to the specified data.
        
        features is a feature matrix and should be a 2D list-like structure.
        
        labels is the list of labels and should have the same number of
        elements as the feature matrix has rows.
        
        training_ratio is a value between 0 and 1 that defines the ratio of
        the given data that will be used for training. The rest will be used
        for testing. The default is 0.7.
        """
        
        model = xgb.XGBRegressor()
        
        super(ExeXGBClassifier, self).__init__(model, features, labels, 
                                               training_ratio)